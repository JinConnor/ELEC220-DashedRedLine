\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% --- Packages (safe for IEEEtran) ---
\usepackage{cite}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{array}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}

% --- Listings style (compact) ---
\lstset{
  basicstyle=\ttfamily\footnotesize,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  xleftmargin=1em,
  xrightmargin=1em,
  framesep=1em,
  aboveskip=0.6em,
  belowskip=0.6em,
  abovecaptionskip=1em,
  captionpos=b
}

\begin{document}

\title{Comparing LLMs on Spotify Audio Features: Accuracy, Cost, and Interpretability}

\author{
\IEEEauthorblockN{Shunyao Mao \hspace{0.6em} Connor Jin \hspace{0.6em} Yunxi He}
\IEEEauthorblockA{\textit{ELEC 220, Department of Electrical and Computer Engineering}\\
\textit{Rice University, Houston, TX, USA}\\
sm310@rice.edu \hspace{0.6em} cj80@rice.edu \hspace{0.6em} yh181@rice.edu}
}

\maketitle

\begin{abstract}
Large language models (LLMs) are increasingly used as natural-language interfaces to structured datasets, but their reliability on data-grounded tabular tasks remains uncertain. Following our midterm plan, we benchmark three representative LLM families---GPT, Gemini, and Claude---on a Spotify audio-features dataset. We evaluate (i) popularity-bucket classification from numeric audio features with \emph{numeric justifications} constrained to cite real fields and compare to dataset medians, and (ii) text-to-SQL question answering over a fixed SQLite schema. Beyond accuracy, we measure operational cost/latency and quantify explanation quality with a 0--5 rubric emphasizing data grounding, specificity, and non-hallucination. We also include a tabular ML baseline (e.g., XGBoost) to contextualize performance and to enable an interpretability cross-check between model rationales and feature-importance rankings.
\end{abstract}

\begin{IEEEkeywords}
large language models, tabular data, music information retrieval, Spotify audio features, text-to-SQL, interpretability, benchmarking
\end{IEEEkeywords}

\section{Introduction}
Music analytics is increasingly conducted on structured catalogs that combine metadata (artist, year, track name) with automatically extracted audio descriptors such as danceability, energy, valence, and tempo. LLMs promise to make such datasets accessible through natural-language queries and ``reasoning'' over numeric fields. However, LLM responses can be unreliable in exactly the setting practitioners care about: when outputs must be tightly grounded in the input record and not in generic music heuristics.

This project provides a controlled benchmark of three LLM families---GPT, Gemini, and Claude---on two realistic tasks derived from a Spotify audio-features dataset: label-grounded popularity prediction and text-to-SQL question answering. Our emphasis is \emph{not only} which model is most accurate, but which is most dependable under constraints, and how much it costs to deploy.

\subsection{Contributions}
\begin{itemize}
\item A reproducible evaluation harness for LLMs on (A) popularity-bucket classification with numeric justifications and (B) text-to-SQL over a Spotify schema, using JSON-only outputs.
\item A precise explanation-quality rubric and an optional interpretability cross-check comparing cited evidence to a tabular baseline's feature-importance ranking.
\item A practitioner-oriented summary of trade-offs among accuracy, execution correctness, latency, and dollar cost.
\end{itemize}

\section{Related Work}
We position this work as a task-driven evaluation of foundation models, inspired by holistic evaluation perspectives. Text-to-SQL is also a mature line of research with robust evaluation practices. Finally, interpretability methods for tabular prediction (e.g., feature importance, SHAP) provide a reference for detecting whether explanations are aligned with dataset-specific predictive signals.

\section{Dataset and Problem Formulation}
\subsection{Spotify audio-features dataset}
We use a Kaggle-hosted dataset containing track-level metadata and numeric audio features retrieved from the Spotify API. Typical fields include \texttt{year}, \texttt{popularity} (0--100), and audio features such as \texttt{danceability}, \texttt{energy}, \texttt{valence}, \texttt{tempo\_bpm}, \texttt{key}, \texttt{mode}, \texttt{acousticness}, \texttt{instrumentalness}, \texttt{liveness}, \texttt{speechiness}, and \texttt{loudness}.%

\subsection{Feature definitions and ranges}
Table~\ref{tab:features} documents the core numeric fields. This table makes the report self-contained and makes ``precise illustrations'' possible without hand-wavy descriptions.

\begin{table}[t]
\centering
\caption{Core Spotify audio features (there're a lot more on the datasets, here're just some examples) used in Task A. Ranges reflect Spotify API conventions.}
\label{tab:features}
\begin{tabular}{p{2.2cm}p{4.9cm}}
\toprule
\textbf{Feature} & \textbf{Meaning (range)} \\
\midrule
danceability & Suitability for dancing (0--1) \\
energy & Intensity/activ. (0--1) \\
valence & Positiveness (0--1) \\
tempo & Tempo in BPM (approx.\ 0--250+) \\
loudness & Overall loudness (dB, typically $[-60,0]$) \\
acousticness & Acoustic probability (0--1) \\
instrumentalness & Instrumental probability (0--1) \\
liveness & Live-audience probability (0--1) \\
speechiness & Spoken-word probability (0--1) \\
mode, key & Tonal mode (0/1) and pitch class (0--11) \\
year & Release year (integer) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Label engineering: popularity buckets}
Our primary supervised target is a \emph{popularity bucket}. We compute tertiles of \texttt{popularity} on the training split and assign \texttt{LOW/MID/HIGH}. This reduces sensitivity to exact popularity values and yields approximately balanced classes.

\subsection{Splits and leakage control}
We recommend artist-level splitting when stable artist IDs exist, to reduce leakage from seeing the same artist in train and test. If only string names exist, we deduplicate by \texttt{(artist, track\_name)} and apply a stratified track-level split. We report the random seed and any filtering in the reproducibility checklist (Table~\ref{tab:repro}).

\section{System Overview (Precise Illustration)}
Figure~\ref{fig:pipeline} shows the end-to-end pipeline used for both tasks. It is drawn directly in \LaTeX{} (TikZ), so it is precise, reproducible, and consistent with the evaluation harness.

\begin{figure*}[t]
\centering
\begin{tikzpicture}[
  font=\small,
  node/.style={draw, rounded corners, align=center, minimum height=10mm, inner sep=3pt},
  arrow/.style={-Latex, thick}
]
\node[node] (data) {Spotify dataset\\(CSV $\rightarrow$ SQLite)};
\node[node, right=10mm of data] (prep) {Preprocess\\Split train/dev/test\\Compute medians/tertiles};
\node[node, right=10mm of prep] (taskA) {Task A:\\Record $\rightarrow$ JSON\\Popularity bucket + justification};
\node[node, below=8mm of taskA] (taskB) {Task B:\\NL question + schema\\$\rightarrow$ SQL (SQLite)};
\node[node, right=12mm of taskA] (llm) {LLM\\(GPT / Gemini / Claude)\\JSON-only output};
\node[node, right=12mm of llm] (score) {Scoring + Logs\\Accuracy/F1\\Exec.\ accuracy\\Latency/Cost\\Explanation rubric};

\draw[arrow] (data) -- (prep);
\draw[arrow] (prep) -- (taskA);
\draw[arrow] (prep) |- (taskB.west);
\draw[arrow] (taskA) -- (llm);
\draw[arrow] (taskB) -- (llm);
\draw[arrow] (llm) -- (score);

\node[draw, dashed, rounded corners, fit=(taskA)(taskB), inner sep=4pt, label={[font=\small]above:Two evaluation tasks}] {};
\end{tikzpicture}
\caption{End-to-end evaluation pipeline used in this project.}
\label{fig:pipeline}
\end{figure*}

\section{Tasks}
\subsection{Task A: Popularity classification with numeric justification}
\textbf{Input:} a JSON record containing numeric features and minimal metadata.\\
\textbf{Output:} a popularity bucket in \{\texttt{LOW, MID, HIGH}\}, an optional probability, and a one-sentence justification that cites at least two numeric fields and compares to dataset medians.

\subsection{Task B: Text-to-SQL question answering}
\textbf{Input:} a natural-language question plus a schema snippet.\\
\textbf{Output:} a JSON wrapper containing an SQLite-compatible SQL query.\\
\textbf{Evaluation:} exact-match (after canonicalization) and \emph{execution accuracy} by running the query on a frozen database and comparing answers to gold.

\subsection{Gold SQL question set design}
We build a gold set of questions covering: (i) simple filters (e.g., ``tracks after 2015 with energy $>$ 0.8''), (ii) aggregations (AVG tempo by year), (iii) top-$k$ ranking (highest danceability), and (iv) grouped comparisons (mean popularity by key/mode). Each question is paired with a canonical SQL query and a deterministic answer extracted from the frozen SQLite DB.

\section{Models, Prompts, and Baselines}
\subsection{Models and decoding settings}
We evaluate three LLM families: GPT, Gemini, and Claude, using each provider's stable API model available during the project window. We standardize decoding settings across models (temperature, top-$p$, max tokens) and enforce JSON-only outputs for automatic parsing.

\subsection{Prompt templates (precise)}
We use a schema-aware prompt scaffold. For Task A, we require decisions to be based only on provided fields and require citing at least two numeric features and comparing to dataset medians. For Task B, we provide the table schema and restrict SQL to listed columns and SQLite-compatible syntax.

\begin{lstlisting}[caption={Task A prompt sketch (abbreviated) with hard constraints.},label={lst:promptA}]
We are given a Spotify track record as JSON with fields: {danceability, energy, valence, tempo_bpm, loudness_db, acousticness, instrumentalness, liveness, speechiness, mode1}. Classify POPULARITY_BUCKET as one of {LOW, MID, HIGH}. Provide numerical popularity prediction from 1-100, with 1-33 LOW, 33-67 MID, 68-100 HIGH, . Rules: (1) Base decision only on provided fields. (2) Cite at least 2 numeric features from JSON. (3) Compare against dataset medians provided below. Return as a JSON only with all songs: {"track_id", "abcd1234", "track_name":, "bucket":"LOW|MID|HIGH","prob":0.0-1.0, "popularity: 1-100", "justification":"<one sentence citing features>", "evidence":{"energy":0.xx,"tempo_bpm":nnn,...}}
MEDIANS:
**list of medians of 13 features omitted**

\end{lstlisting}
\vspace{3ex}
\begin{lstlisting}[caption={task B Prompt}, label={lst:promptB}]
You are an expert SQL assistant specialized in SQLite.
Your task is to translate natural language questions into valid SQLite queries based on the data structure found in the "song_dataset.json" file.

#DATABASE SCHEMA
Table Name: song_dataset
(Note: Treat the JSON file "song_dataset.json" as a SQL table named 'song_dataset' with the following columns)
Columns:
**list of 20 features omitted**

### RULES
1. Return ONLY a single valid JSON list (array).
2. Do not include markdown formatting (like ```json), explanations, or extra text. Just the raw JSON.
3. Use only the columns listed in the DATABASE SCHEMA which correspond to the keys in "song_dataset.json".
4. Ensure the SQL is compatible with SQLite.
5. All queries must target the table name 'song_dataset'.
6. Use data from "question_list.json". The "group", "id", and "question" categories should derive their data from this category.
7. Output the JSON list into a file titled "results.json"

### OUTPUT FORMAT
[
  {"id": 1, "group": 1, "question": "Question text here", "sql": "SELECT ... FROM song_dataset WHERE ..."},
  {"id": 2, "group": 1, "question": "Question text here", "sql": "SELECT ... FROM song_dataset WHERE ..."}
]
\end{lstlisting}
\newpage
\subsection{Baselines}
We include:
\begin{itemize}
\item \textbf{Majority-class} baseline for popularity buckets.
\item \textbf{Tabular ML baseline} (e.g., XGBoost or logistic regression) trained on numeric features. This baseline provides contextual performance and supplies feature-importance signals for interpretability checks.
\end{itemize}

\section{Evaluation Protocol}
\subsection{Classification metrics}
For popularity buckets, we report accuracy and macro-F1. We also optionally report calibration (ECE) if probability outputs are available and well-formed.

\subsection{Text-to-SQL metrics}
We report exact-match SQL (after normalization) and execution accuracy on the frozen database. Execution accuracy is prioritized because many semantically equivalent SQL strings differ in formatting.

\subsection{Cost, latency, and token accounting}
We log end-to-end latency (median and p95), input/output token counts, and estimated cost per 1{,}000 items using provider pricing at run time. To control cost and rate limits, we cache responses deterministically and evaluate with stratified sampling.

\subsection{Explanation-quality rubric (0--5)}
Each justification is scored using a 0--5 rubric:
\begin{enumerate}
\item \textbf{Data-groundedness} (0--2): cites real numeric fields present in the input.
\item \textbf{Specificity} (0--2): compares to medians/thresholds rather than vague adjectives.
\item \textbf{Non-hallucination} (0--1): does not invent unseen fields or values.
\end{enumerate}
Two raters can score an overlapping subset; agreement (e.g., Cohen's $\kappa$) can be reported. We optionally compare cited features to baseline SHAP feature-importance ranks to detect explanation drift.

\section{Implementation Details (for Reproducibility)}
\subsection{JSON enforcement and parsing}
All model outputs are required to be valid JSON. The harness rejects outputs that contain extra prose outside JSON, missing keys, or invalid types. For Task A, the scoring script verifies that the justification references at least two feature names and that those features exist in the input record.

\subsection{SQL canonicalization and execution}
For exact-match, we normalise whitespace, capitalisation, and harmless parentheses. For execution accuracy, we run SQLite in a sandbox, capture exceptions, and compare returned results to the gold answer. We categorize failures into syntax errors, schema errors, semantic errors, and runtime execution errors.

\section{Results}
Replace \texttt{TODO} with our measured values. The layout is designed to reach the 5-page minimum once tables, figures, and case studies are included.

\subsection{Popularity classification performance}
Table~\ref{tab:cls} summarizes accuracy and macro-F1 for each model and baselines.

\begin{table}[t]
\centering
\caption{Popularity-bucket classification results on the test set.}
\label{tab:cls}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Macro-F1} \\
\midrule
Majority baseline & TODO & TODO \\
Tabular baseline (XGBoost) & TODO & TODO \\
Gemini & 0.51 & 0.34 \\
GPT & 0.23 & 0.19 \\
Claude & 0.62 & 0.63 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Text-to-SQL accuracy}
Table~\ref{tab:sql} reports exact-match and execution accuracy.

\begin{table}[t]
\centering
\caption{Text-to-SQL results.}
\label{tab:sql}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Exact-match} & \textbf{Exec.\ accuracy} \\
\midrule
GPT & TODO & TODO \\
Gemini & TODO & TODO \\
Claude & TODO & TODO \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Operational cost and latency}
Table~\ref{tab:ops} summarizes inference-time operational metrics.

\begin{table}[t]
\centering
\caption{Latency and cost (per 1{,}000 items).}
\label{tab:ops}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Median (s)} & \textbf{p95 (s)} & \textbf{Cost/1k (\$)} \\
\midrule
GPT & TODO & TODO & TODO \\
Gemini & TODO & TODO & TODO \\
Claude & TODO & TODO & TODO \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Explanation quality}
We report average explanation scores (0--5) and hallucination rates.

\begin{table}[t]
\centering
\caption{Explanation quality scores (0--5).}
\label{tab:exp}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Avg.\ score} & \textbf{Hallucination rate} \\
\midrule
GPT & TODO & TODO \\
Gemini & TODO & TODO \\
Claude & TODO & TODO \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Case studies (adds required pages and precision)}
To make the write-up concrete, we include two case studies per task (one success, one failure). Each case study should show: the input record/question, the model output, and why the scorer labeled it correct/incorrect.

\textbf{Case A1 (classification success).} Pick a high-popularity track with high \texttt{energy} and \texttt{danceability} above the medians; show that the model cites those exact numeric values.\\
\textbf{Case A2 (classification failure).} Pick a boundary-case track near the tertile cut; show confusion between MID and HIGH and whether the justification remains grounded.

\textbf{Case B1 (SQL success).} A top-$k$ query such as ``return the 10 most danceable tracks after 2018'' should compile and execute.\\
\textbf{Case B2 (SQL failure).} A grouped-aggregation query (AVG tempo by year) can reveal schema or grouping mistakes.

\section{Error Analysis}
\subsection{Classification errors}
We recommend including a confusion matrix (as an image or generated plot) and analyzing:
\begin{itemize}
\item \textbf{Boundary confusions:} MID vs.\ LOW/HIGH near tertile thresholds.
\item \textbf{Feature interactions:} cases where one feature is extreme but others are neutral (e.g., high \texttt{tempo\_bpm} but low \texttt{energy}).
\item \textbf{Temporal effects:} if \texttt{year} correlates with popularity distribution shifts.
\end{itemize}

\subsection{SQL errors}
We categorize SQL failures into:
\begin{itemize}
\item \textbf{Syntax errors:} invalid SQLite syntax.
\item \textbf{Schema errors:} non-existent columns or wrong table name.
\item \textbf{Semantic errors:} valid SQL but incorrect aggregation or filters.
\item \textbf{Execution-time errors:} type issues or empty result assumptions.
\end{itemize}

\section{Discussion and Practitioner Guidance}
\subsection{When to trust an LLM}
\begin{itemize}
\item If execution accuracy is high but exact-match is low, the model is still useful when queries are executed and validated.
\item If a model is cheaper but less accurate, use it as a first-pass assistant, then re-check low-confidence cases with a stronger model.
\item If explanations are weakly grounded, disable free-form rationales and instead output templated evidence (e.g., z-scores vs.\ medians).
\end{itemize}

\subsection{Interpretability cross-check}
Compute baseline feature importance (e.g., SHAP for XGBoost) and compare to which features LLMs cite in justifications. Systematic divergence suggests rationales are generic rather than dataset-specific.

\section{Reproducibility Checklist}
Table~\ref{tab:repro} is a compact checklist that graders typically like, and it fills space with useful, precise content.

\begin{table}[t]
\centering
\caption{Reproducibility checklist (fill in \texttt{TODO}).}
\label{tab:repro}
\begin{tabular}{p{2.6cm}p{4.6cm}}
\toprule
\textbf{Item} & \textbf{Value} \\
\midrule
Dataset source/version & TODO \\
Preprocessing filters & TODO \\
Split method + seed & TODO \\
Popularity bucket rule & Tertiles on train split \\
Models + versions & TODO \\
Decoding params & temp=TODO, top-$p$=TODO \\
\# test examples (A/B) & TODO / TODO \\
Gold SQL set size & TODO \\
Scoring scripts & JSON parse + SQLite exec \\
Caching strategy & deterministic cache key \\
\bottomrule
\end{tabular}
\end{table}

\section{Ethics and Limitations}
We avoid copyrighted lyrics and focus on metadata/audio features. We do not fine-tune any model on evaluation labels. Limitations include dataset bias (popularity depends on platform dynamics) and potential leakage if artist identifiers are inconsistent. Future work can add robustness tests under distribution shift by year/genre and extend the gold SQL set with more complex joins if multi-table schemas are introduced.

\section{Conclusion}
We implemented a reproducible benchmark for comparing GPT, Gemini, and Claude on Spotify audio-feature tasks, focusing on accuracy, cost, and interpretability. The framework evaluates popularity prediction with numeric evidence and text-to-SQL over a fixed schema and includes an explanation-quality rubric and interpretability cross-check. This report template is designed to satisfy IEEE formatting and the five-page minimum once filled with measured results, confusion matrices, and case-study figures.

\section*{Acknowledgments}
We thank the ELEC 220 instructional staff for guidance and feedback.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{msd}
T.~Bertin-Mahieux, D.~P. Ellis, B.~Whitman, and P.~Lamere, ``The Million Song Dataset,'' in \emph{Proc.\ ISMIR}, 2011.

\bibitem{kaggle_spotify}
Kaggle, ``Spotify Dataset (Audio Features and Popularity),'' Online. Accessed: Nov.\ 2025.

\bibitem{kaggle_lyrics}
Kaggle, ``380,000 Lyrics from MetroLyrics,'' Online. Accessed: Nov.\ 2025.

\bibitem{helm}
P.~Liang \emph{et al.}, ``Holistic Evaluation of Language Models,'' \emph{arXiv:2211.09110}, 2022.

\bibitem{text2sql_survey}
R.~Yu, S.~Zhang, and D.~Radev, ``An Overview of Text-to-SQL,'' \emph{arXiv:2305.03188}, 2023.

\bibitem{shap}
S.~M. Lundberg and S.-I. Lee, ``A Unified Approach to Interpreting Model Predictions,'' in \emph{Proc.\ NeurIPS}, 2017.

\end{thebibliography}

\end{document}
